{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing the CSV files\n",
    "results_dir = '../Results/UserStudy'\n",
    "\n",
    "# List to hold data from all CSV files\n",
    "all_responses = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        # Read the CSV file and append its data to the list\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        all_responses.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "merged_responses = pd.concat(all_responses, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('Web/mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_letters_to_models(merged_responses, mapping):\n",
    "    # Iterate over each row in merged_responses\n",
    "    for index, row in merged_responses.iterrows():\n",
    "        # Find the corresponding row in mapping based on text_prompt\n",
    "        mapping_row = mapping[mapping['text_prompt'] == row['text_prompt']].iloc[0]\n",
    "        \n",
    "        # Create a dictionary to map letters to model URLs\n",
    "        letter_to_model = {\n",
    "            'A': mapping_row['model_url_1'],\n",
    "            'B': mapping_row['model_url_2'],\n",
    "            'C': mapping_row['model_url_3'],\n",
    "            'D': mapping_row['model_url_4']\n",
    "        }\n",
    "        \n",
    "        # Replace letters in each ranking column with model names\n",
    "        for column in ['smoothness_ranking', 'picture_quality_ranking', 'accuracy_ranking', 'overall_ranking']:\n",
    "            if pd.notna(row[column]):\n",
    "                merged_responses.at[index, column] = ','.join([letter_to_model[letter] for letter in row[column].split(',')])\n",
    "    return merged_responses\n",
    "\n",
    "def calculate_model_scores(merged_responses):\n",
    "    # Initialize dictionaries to hold scores and all rankings for each model\n",
    "    model_scores = {\n",
    "        'smoothness_ranking': {},\n",
    "        'picture_quality_ranking': {},\n",
    "        'accuracy_ranking': {},\n",
    "        'overall_ranking': {}\n",
    "    }\n",
    "    model_all_rankings = {\n",
    "        'smoothness_ranking': {},\n",
    "        'picture_quality_ranking': {},\n",
    "        'accuracy_ranking': {},\n",
    "        'overall_ranking': {}\n",
    "    }\n",
    "\n",
    "    # Calculate scores and store all rankings for each column\n",
    "    for index, row in merged_responses.iterrows():\n",
    "        for column in ['smoothness_ranking', 'picture_quality_ranking', 'accuracy_ranking', 'overall_ranking']:\n",
    "            if pd.notna(row[column]):\n",
    "                models = row[column].split(',')\n",
    "                for position, model in enumerate(models):\n",
    "                    points = position + 1\n",
    "                    \n",
    "                    # Initialize lists if model not present\n",
    "                    if model not in model_scores[column]:\n",
    "                        model_scores[column][model] = 0\n",
    "                        model_all_rankings[column][model] = []\n",
    "                    \n",
    "                    model_scores[column][model] += points\n",
    "                    model_all_rankings[column][model].append(points)\n",
    "\n",
    "    # Calculate mean and std for each model\n",
    "    result = {\n",
    "        'mean': model_scores,\n",
    "        'std': {k: {} for k in model_scores.keys()}\n",
    "    }\n",
    "    \n",
    "    for column in model_scores.keys():\n",
    "        total_rankings = len(merged_responses[column].dropna())\n",
    "        for model in model_scores[column]:\n",
    "            # Calculate mean\n",
    "            result['mean'][column][model] /= total_rankings\n",
    "            # Calculate std\n",
    "            result['std'][column][model] = np.std(model_all_rankings[column][model])\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rankings: 400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of rankings: {len(merged_responses['smoothness_ranking'].dropna())}\")\n",
    "merged_responses_mapped = map_letters_to_models(merged_responses, mapping)\n",
    "model_scores = calculate_model_scores(merged_responses_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings for smoothness_ranking:\n",
      "       Model Mean   Std\n",
      "0  FreeBloom  2.5  1.05\n",
      "1    T2VZero  2.4  1.10\n",
      "2      EIDTV  1.9  0.93\n",
      "3   DirecT2V  3.2  0.97\n",
      "\n",
      "Rankings for picture_quality_ranking:\n",
      "       Model Mean   Std\n",
      "0  FreeBloom  2.0  0.93\n",
      "1    T2VZero  2.5  1.00\n",
      "2      EIDTV  2.0  0.97\n",
      "3   DirecT2V  3.5  0.81\n",
      "\n",
      "Rankings for accuracy_ranking:\n",
      "       Model Mean   Std\n",
      "0  FreeBloom  2.0  1.01\n",
      "1   DirecT2V  3.1  0.99\n",
      "2      EIDTV  2.1  1.04\n",
      "3    T2VZero  2.7  1.09\n",
      "\n",
      "Rankings for overall_ranking:\n",
      "       Model Mean   Std\n",
      "0  FreeBloom  2.3  1.00\n",
      "1      EIDTV  1.9  0.97\n",
      "2    T2VZero  2.5  1.04\n",
      "3   DirecT2V  3.3  0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the print function to include std\n",
    "def print_scores_as_table(model_scores):\n",
    "    for ranking in model_scores['mean'].keys():\n",
    "        print(f\"Rankings for {ranking}:\")\n",
    "        # Create DataFrame with both mean and std\n",
    "        scores_data = []\n",
    "        for model in model_scores['mean'][ranking].keys():\n",
    "            scores_data.append({\n",
    "                'Model': model,\n",
    "                'Mean': f\"{model_scores['mean'][ranking][model]:.1f}\",\n",
    "                'Std': f\"{model_scores['std'][ranking][model]:.2f}\"\n",
    "            })\n",
    "        df_scores = pd.DataFrame(scores_data)\n",
    "        print(df_scores)\n",
    "        print()\n",
    "\n",
    "print_scores_as_table(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIDT_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
